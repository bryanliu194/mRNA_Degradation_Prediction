{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/bryan194/ae-pretrain-gnn-attn-cnn?scriptVersionId=114284532\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"### training scheme\n#### (1) train denoising auto encoder model using all data including train and test data\n#### (2) from the weights of denoising auto encoder model, finetune to predict targets such as reactivity\n\n### rough network architecture\n#### inputs -> conv1ds -> aggregation of neighborhoods -> multi head attention -> aggregation of neighborhoods -> multi head attention -> conv1d -> predict\n#### this architecture was inspired by https://www.kaggle.com/cpmpml/graph-transfomer\n","metadata":{}},{"cell_type":"code","source":"pretrain_dir = None # model dir for resuming training. if None, train from scrach\n\none_fold = False # if True, train model at only first fold. use if you try a new idea quickly.\nrun_test = False # if True, use small data. you can check whether this code run or not\ndenoise = True # if True, use train data whose signal_to_noise > 1\n\nae_epochs = 20 # epoch of training of denoising auto encoder\nae_epochs_each = 5 # epoch of training of denoising auto encoder each time. \n                   # I use train data (seqlen = 107) and private test data (seqlen = 130) for auto encoder training.\n                   # I dont know how to easily fit keras model to use both of different shape data simultaneously, \n                   # so I call fit function several times. \nae_batch_size = 32\n\nepochs_list = [30, 10, 3, 3, 5, 5]\nbatch_size_list = [8, 16, 32, 64, 128, 256] \n\n## copy pretrain model to working dir\nimport shutil\nimport glob\nif pretrain_dir is not None:\n    for d in glob.glob(pretrain_dir + \"*\"):\n        shutil.copy(d, \".\")\n    \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gc\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-10-20T05:38:11.422798Z","iopub.execute_input":"2022-10-20T05:38:11.423138Z","iopub.status.idle":"2022-10-20T05:38:11.435055Z","shell.execute_reply.started":"2022-10-20T05:38:11.423107Z","shell.execute_reply":"2022-10-20T05:38:11.434084Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## load","metadata":{"trusted":true}},{"cell_type":"code","source":"import json\nimport glob\nfrom tqdm.notebook import tqdm\n\ntrain = pd.read_json(\"/kaggle/input/stanford-covid-vaccine/train.json\",lines=True)\nif denoise:\n    train = train[train.signal_to_noise > 1].reset_index(drop = True)\ntest  = pd.read_json(\"/kaggle/input/stanford-covid-vaccine/test.json\",lines=True)\ntest_pub = test[test[\"seq_length\"] == 107]\ntest_pri = test[test[\"seq_length\"] == 130]\nsub = pd.read_csv(\"/kaggle/input/stanford-covid-vaccine/sample_submission.csv\")\n\nif run_test: ## to test \n    train = train[:30]\n    test_pub = test_pub[:30]\n    test_pri = test_pri[:30]\n\nAs = []\nfor id in tqdm(train[\"id\"]):\n    a = np.load(f\"/kaggle/input/stanford-covid-vaccine/bpps/{id}.npy\")\n    As.append(a)\nAs = np.array(As)\nAs_pub = []\nfor id in tqdm(test_pub[\"id\"]):\n    a = np.load(f\"/kaggle/input/stanford-covid-vaccine/bpps/{id}.npy\")\n    As_pub.append(a)\nAs_pub = np.array(As_pub)\nAs_pri = []\nfor id in tqdm(test_pri[\"id\"]):\n    a = np.load(f\"/kaggle/input/stanford-covid-vaccine/bpps/{id}.npy\")\n    As_pri.append(a)\nAs_pri = np.array(As_pri)","metadata":{"execution":{"iopub.status.busy":"2022-10-20T05:38:11.443126Z","iopub.execute_input":"2022-10-20T05:38:11.443455Z","iopub.status.idle":"2022-10-20T05:39:11.963393Z","shell.execute_reply.started":"2022-10-20T05:38:11.443421Z","shell.execute_reply":"2022-10-20T05:39:11.962471Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=2096.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"405d7cf290d147298ce5ba17fa2a9446"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=629.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ada5f7bf42f8485191bfd270323cd818"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=3005.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d4c23e9fafd4161a68735346fc229af"}},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"print(train.shape)\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-20T05:39:11.965334Z","iopub.execute_input":"2022-10-20T05:39:11.965841Z","iopub.status.idle":"2022-10-20T05:39:12.052729Z","shell.execute_reply.started":"2022-10-20T05:39:11.965799Z","shell.execute_reply":"2022-10-20T05:39:12.05167Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"(2096, 19)\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   index            id                                           sequence  \\\n0      0  id_001f94081  GGAAAAGCUCUAAUAACAGGAGACUAGGACUACGUAUUUCUAGGUA...   \n1      2  id_006f36f57  GGAAAGUGCUCAGAUAAGCUAAGCUCGAAUAGCAAUCGAAUAGAAU...   \n2      5  id_00ab2d761  GGAAAGCGCCGCGGCGGUAGCGGCAGCGAGGAGCGCUACCAAGGCA...   \n3      6  id_00abef1d7  GGAAAACAAUUGCAUCGUUAGUACGACUCCACAGCGUAAGCUGUGG...   \n4      7  id_00b436dec  GGAAAUCAUCGAGGACGGGUCCGUUCAGCACGCGAAAGCGUCGUGA...   \n\n                                           structure  \\\n0  .....((((((.......)))).)).((.....((..((((((......   \n1  .....((((.((.....((((.(((.....)))..((((......)...   \n2  .....(.(((((.(((((((((...........)))))))..(((....   \n3  .........((((((((......((((((((((((....)))))))...   \n4  .....(((((((((((..(((((((((..((((....))))..)))...   \n\n                                 predicted_loop_type  signal_to_noise  \\\n0  EEEEESSSSSSHHHHHHHSSSSBSSXSSIIIIISSIISSSSSSHHH...            6.894   \n1  EEEEESSSSISSIIIIISSSSMSSSHHHHHSSSMMSSSSHHHHHHS...            8.800   \n2  EEEEESISSSSSISSSSSSSSSHHHHHHHHHHHSSSSSSSMMSSSH...            4.136   \n3  EEEEEEEEESSSSSSSSIIIIIISSSSSSSSSSSSHHHHSSSSSSS...            2.485   \n4  EEEEESSSSSSSSSSSIISSSSSSSSSIISSSSHHHHSSSSIISSS...            1.727   \n\n   SN_filter  seq_length  seq_scored  \\\n0          1         107          68   \n1          1         107          68   \n2          1         107          68   \n3          1         107          68   \n4          1         107          68   \n\n                                    reactivity_error  \\\n0  [0.1359, 0.20700000000000002, 0.1633, 0.1452, ...   \n1  [0.0931, 0.13290000000000002, 0.11280000000000...   \n2  [0.1942, 0.2041, 0.1626, 0.1213, 0.10590000000...   \n3  [0.422, 0.5478000000000001, 0.4749000000000000...   \n4  [0.4843, 0.5233, 0.4554, 0.43520000000000003, ...   \n\n                                   deg_error_Mg_pH10  \\\n0  [0.26130000000000003, 0.38420000000000004, 0.1...   \n1  [0.1365, 0.2237, 0.1812, 0.1333, 0.1148, 0.160...   \n2  [0.2726, 0.2984, 0.21660000000000001, 0.1637, ...   \n3  [0.4801, 0.7943, 0.42160000000000003, 0.397300...   \n4  [0.8719, 1.0307, 0.6649, 0.34500000000000003, ...   \n\n                                      deg_error_pH10  \\\n0  [0.2631, 0.28600000000000003, 0.0964, 0.1574, ...   \n1  [0.17020000000000002, 0.178, 0.111, 0.091, 0.0...   \n2  [0.3393, 0.2728, 0.2005, 0.1703, 0.1495, 0.134...   \n3  [0.9822000000000001, 1.272, 0.6940000000000001...   \n4  [0.7045, 0.7775000000000001, 0.5662, 0.4561, 0...   \n\n                                    deg_error_Mg_50C  \\\n0  [0.1501, 0.275, 0.0947, 0.18660000000000002, 0...   \n1  [0.1033, 0.1464, 0.1126, 0.09620000000000001, ...   \n2  [0.165, 0.20520000000000002, 0.179, 0.1333, 0....   \n3  [0.5827, 0.7555000000000001, 0.5949, 0.4511, 0...   \n4  [0.384, 0.723, 0.4766, 0.30260000000000004, 0....   \n\n                                       deg_error_50C  \\\n0  [0.2167, 0.34750000000000003, 0.188, 0.2124, 0...   \n1  [0.14980000000000002, 0.1761, 0.1517, 0.116700...   \n2  [0.2864, 0.24710000000000001, 0.2222, 0.1903, ...   \n3  [0.9306000000000001, 1.0496, 0.5844, 0.7796000...   \n4  [0.7429, 0.9137000000000001, 0.480400000000000...   \n\n                                          reactivity  \\\n0  [0.3297, 1.5693000000000001, 1.1227, 0.8686, 0...   \n1  [0.44820000000000004, 1.4822, 1.1819, 0.743400...   \n2  [0.7642, 1.6641, 1.0622, 0.5008, 0.4107, 0.133...   \n3  [0.895, 2.3377, 2.2305, 2.003, 1.9006, 1.0373,...   \n4  [1.1576, 1.5137, 1.3382, 1.5622, 1.2121, 0.295...   \n\n                                         deg_Mg_pH10  \\\n0  [0.7556, 2.983, 0.2526, 1.3789, 0.637600000000...   \n1  [0.2504, 1.4021, 0.9804, 0.49670000000000003, ...   \n2  [0.9559000000000001, 1.9442, 1.0114, 0.5105000...   \n3  [0.46040000000000003, 3.6695, 0.78550000000000...   \n4  [1.6912, 5.2652, 2.3901, 0.45890000000000003, ...   \n\n                                            deg_pH10  \\\n0  [2.3375, 3.5060000000000002, 0.3008, 1.0108, 0...   \n1  [2.243, 2.9361, 1.0553, 0.721, 0.6396000000000...   \n2  [1.9554, 2.1298, 1.0403, 0.609, 0.5486, 0.386,...   \n3  [2.7711, 7.365, 1.6924000000000001, 1.43840000...   \n4  [1.8641, 2.3767, 1.149, 1.0132, 0.9876, 0.0, 0...   \n\n                                          deg_Mg_50C  \\\n0  [0.35810000000000003, 2.9683, 0.2589, 1.4552, ...   \n1  [0.5163, 1.6823000000000001, 1.0426, 0.7902, 0...   \n2  [0.22460000000000002, 1.7281, 1.381, 0.6623, 0...   \n3  [1.073, 2.8604000000000003, 1.9936, 1.0273, 1....   \n4  [0.49060000000000004, 4.6339, 1.95860000000000...   \n\n                                             deg_50C  \n0  [0.6382, 3.4773, 0.9988, 1.3228, 0.78770000000...  \n1  [0.9501000000000001, 1.7974999999999999, 1.499...  \n2  [0.5882000000000001, 1.1786, 0.9704, 0.6035, 0...  \n3  [2.0964, 3.3688000000000002, 0.6399, 2.1053, 1...  \n4  [1.2852000000000001, 2.5460000000000003, 0.234...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>id</th>\n      <th>sequence</th>\n      <th>structure</th>\n      <th>predicted_loop_type</th>\n      <th>signal_to_noise</th>\n      <th>SN_filter</th>\n      <th>seq_length</th>\n      <th>seq_scored</th>\n      <th>reactivity_error</th>\n      <th>deg_error_Mg_pH10</th>\n      <th>deg_error_pH10</th>\n      <th>deg_error_Mg_50C</th>\n      <th>deg_error_50C</th>\n      <th>reactivity</th>\n      <th>deg_Mg_pH10</th>\n      <th>deg_pH10</th>\n      <th>deg_Mg_50C</th>\n      <th>deg_50C</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>id_001f94081</td>\n      <td>GGAAAAGCUCUAAUAACAGGAGACUAGGACUACGUAUUUCUAGGUA...</td>\n      <td>.....((((((.......)))).)).((.....((..((((((......</td>\n      <td>EEEEESSSSSSHHHHHHHSSSSBSSXSSIIIIISSIISSSSSSHHH...</td>\n      <td>6.894</td>\n      <td>1</td>\n      <td>107</td>\n      <td>68</td>\n      <td>[0.1359, 0.20700000000000002, 0.1633, 0.1452, ...</td>\n      <td>[0.26130000000000003, 0.38420000000000004, 0.1...</td>\n      <td>[0.2631, 0.28600000000000003, 0.0964, 0.1574, ...</td>\n      <td>[0.1501, 0.275, 0.0947, 0.18660000000000002, 0...</td>\n      <td>[0.2167, 0.34750000000000003, 0.188, 0.2124, 0...</td>\n      <td>[0.3297, 1.5693000000000001, 1.1227, 0.8686, 0...</td>\n      <td>[0.7556, 2.983, 0.2526, 1.3789, 0.637600000000...</td>\n      <td>[2.3375, 3.5060000000000002, 0.3008, 1.0108, 0...</td>\n      <td>[0.35810000000000003, 2.9683, 0.2589, 1.4552, ...</td>\n      <td>[0.6382, 3.4773, 0.9988, 1.3228, 0.78770000000...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>id_006f36f57</td>\n      <td>GGAAAGUGCUCAGAUAAGCUAAGCUCGAAUAGCAAUCGAAUAGAAU...</td>\n      <td>.....((((.((.....((((.(((.....)))..((((......)...</td>\n      <td>EEEEESSSSISSIIIIISSSSMSSSHHHHHSSSMMSSSSHHHHHHS...</td>\n      <td>8.800</td>\n      <td>1</td>\n      <td>107</td>\n      <td>68</td>\n      <td>[0.0931, 0.13290000000000002, 0.11280000000000...</td>\n      <td>[0.1365, 0.2237, 0.1812, 0.1333, 0.1148, 0.160...</td>\n      <td>[0.17020000000000002, 0.178, 0.111, 0.091, 0.0...</td>\n      <td>[0.1033, 0.1464, 0.1126, 0.09620000000000001, ...</td>\n      <td>[0.14980000000000002, 0.1761, 0.1517, 0.116700...</td>\n      <td>[0.44820000000000004, 1.4822, 1.1819, 0.743400...</td>\n      <td>[0.2504, 1.4021, 0.9804, 0.49670000000000003, ...</td>\n      <td>[2.243, 2.9361, 1.0553, 0.721, 0.6396000000000...</td>\n      <td>[0.5163, 1.6823000000000001, 1.0426, 0.7902, 0...</td>\n      <td>[0.9501000000000001, 1.7974999999999999, 1.499...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>id_00ab2d761</td>\n      <td>GGAAAGCGCCGCGGCGGUAGCGGCAGCGAGGAGCGCUACCAAGGCA...</td>\n      <td>.....(.(((((.(((((((((...........)))))))..(((....</td>\n      <td>EEEEESISSSSSISSSSSSSSSHHHHHHHHHHHSSSSSSSMMSSSH...</td>\n      <td>4.136</td>\n      <td>1</td>\n      <td>107</td>\n      <td>68</td>\n      <td>[0.1942, 0.2041, 0.1626, 0.1213, 0.10590000000...</td>\n      <td>[0.2726, 0.2984, 0.21660000000000001, 0.1637, ...</td>\n      <td>[0.3393, 0.2728, 0.2005, 0.1703, 0.1495, 0.134...</td>\n      <td>[0.165, 0.20520000000000002, 0.179, 0.1333, 0....</td>\n      <td>[0.2864, 0.24710000000000001, 0.2222, 0.1903, ...</td>\n      <td>[0.7642, 1.6641, 1.0622, 0.5008, 0.4107, 0.133...</td>\n      <td>[0.9559000000000001, 1.9442, 1.0114, 0.5105000...</td>\n      <td>[1.9554, 2.1298, 1.0403, 0.609, 0.5486, 0.386,...</td>\n      <td>[0.22460000000000002, 1.7281, 1.381, 0.6623, 0...</td>\n      <td>[0.5882000000000001, 1.1786, 0.9704, 0.6035, 0...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>id_00abef1d7</td>\n      <td>GGAAAACAAUUGCAUCGUUAGUACGACUCCACAGCGUAAGCUGUGG...</td>\n      <td>.........((((((((......((((((((((((....)))))))...</td>\n      <td>EEEEEEEEESSSSSSSSIIIIIISSSSSSSSSSSSHHHHSSSSSSS...</td>\n      <td>2.485</td>\n      <td>1</td>\n      <td>107</td>\n      <td>68</td>\n      <td>[0.422, 0.5478000000000001, 0.4749000000000000...</td>\n      <td>[0.4801, 0.7943, 0.42160000000000003, 0.397300...</td>\n      <td>[0.9822000000000001, 1.272, 0.6940000000000001...</td>\n      <td>[0.5827, 0.7555000000000001, 0.5949, 0.4511, 0...</td>\n      <td>[0.9306000000000001, 1.0496, 0.5844, 0.7796000...</td>\n      <td>[0.895, 2.3377, 2.2305, 2.003, 1.9006, 1.0373,...</td>\n      <td>[0.46040000000000003, 3.6695, 0.78550000000000...</td>\n      <td>[2.7711, 7.365, 1.6924000000000001, 1.43840000...</td>\n      <td>[1.073, 2.8604000000000003, 1.9936, 1.0273, 1....</td>\n      <td>[2.0964, 3.3688000000000002, 0.6399, 2.1053, 1...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>id_00b436dec</td>\n      <td>GGAAAUCAUCGAGGACGGGUCCGUUCAGCACGCGAAAGCGUCGUGA...</td>\n      <td>.....(((((((((((..(((((((((..((((....))))..)))...</td>\n      <td>EEEEESSSSSSSSSSSIISSSSSSSSSIISSSSHHHHSSSSIISSS...</td>\n      <td>1.727</td>\n      <td>1</td>\n      <td>107</td>\n      <td>68</td>\n      <td>[0.4843, 0.5233, 0.4554, 0.43520000000000003, ...</td>\n      <td>[0.8719, 1.0307, 0.6649, 0.34500000000000003, ...</td>\n      <td>[0.7045, 0.7775000000000001, 0.5662, 0.4561, 0...</td>\n      <td>[0.384, 0.723, 0.4766, 0.30260000000000004, 0....</td>\n      <td>[0.7429, 0.9137000000000001, 0.480400000000000...</td>\n      <td>[1.1576, 1.5137, 1.3382, 1.5622, 1.2121, 0.295...</td>\n      <td>[1.6912, 5.2652, 2.3901, 0.45890000000000003, ...</td>\n      <td>[1.8641, 2.3767, 1.149, 1.0132, 0.9876, 0.0, 0...</td>\n      <td>[0.49060000000000004, 4.6339, 1.95860000000000...</td>\n      <td>[1.2852000000000001, 2.5460000000000003, 0.234...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(test.shape)\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-20T05:39:12.05422Z","iopub.execute_input":"2022-10-20T05:39:12.054563Z","iopub.status.idle":"2022-10-20T05:39:12.070246Z","shell.execute_reply.started":"2022-10-20T05:39:12.054511Z","shell.execute_reply":"2022-10-20T05:39:12.068975Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"(3634, 7)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   index            id                                           sequence  \\\n0      0  id_00073f8be  GGAAAAGUACGACUUGAGUACGGAAAACGUACCAACUCGAUUAAAA...   \n1      1  id_000ae4237  GGAAACGGGUUCCGCGGAUUGCUGCUAAUAAGAGUAAUCUCUAAAU...   \n2      2  id_00131c573  GGAAAACAAAACGGCCUGGAAGACGAAGGAAUUCGGCGCGAAGGCC...   \n3      3  id_00181fd34  GGAAAGGAUCUCUAUCGAAGGAUAGAGAUCGCUCGCGACGGCACGA...   \n4      4  id_0020473f7  GGAAACCCGCCCGCGCCCGCCCGCGCUGCUGCCGUGCCUCCUCUCC...   \n\n                                           structure  \\\n0  ......((((((((((.(((((.....))))))))((((((((......   \n1  .....((((..((((((...(((((.....((((....)))).......   \n2  ...........((.(((.(.(..((..((..((((...))))..))...   \n3  ......((((((((((....))))))))))((((((..((.(((.....   \n4  .....(((((((((((((((((((((((((((((((((((((((((...   \n\n                                 predicted_loop_type  seq_length  seq_scored  \n0  EEEEEESSSSSSSSSSBSSSSSHHHHHSSSSSSSSSSSSSSSSHHH...         107          68  \n1  EEEEESSSSIISSSSSSIIISSSSSIIIIISSSSHHHHSSSSIIII...         130          91  \n2  EEEEEEEEEEESSISSSISISIISSIISSIISSSSHHHSSSSIISS...         107          68  \n3  EEEEEESSSSSSSSSSHHHHSSSSSSSSSSSSSSSSIISSISSSHH...         107          68  \n4  EEEEESSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS...         130          91  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>id</th>\n      <th>sequence</th>\n      <th>structure</th>\n      <th>predicted_loop_type</th>\n      <th>seq_length</th>\n      <th>seq_scored</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>id_00073f8be</td>\n      <td>GGAAAAGUACGACUUGAGUACGGAAAACGUACCAACUCGAUUAAAA...</td>\n      <td>......((((((((((.(((((.....))))))))((((((((......</td>\n      <td>EEEEEESSSSSSSSSSBSSSSSHHHHHSSSSSSSSSSSSSSSSHHH...</td>\n      <td>107</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>id_000ae4237</td>\n      <td>GGAAACGGGUUCCGCGGAUUGCUGCUAAUAAGAGUAAUCUCUAAAU...</td>\n      <td>.....((((..((((((...(((((.....((((....)))).......</td>\n      <td>EEEEESSSSIISSSSSSIIISSSSSIIIIISSSSHHHHSSSSIIII...</td>\n      <td>130</td>\n      <td>91</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>id_00131c573</td>\n      <td>GGAAAACAAAACGGCCUGGAAGACGAAGGAAUUCGGCGCGAAGGCC...</td>\n      <td>...........((.(((.(.(..((..((..((((...))))..))...</td>\n      <td>EEEEEEEEEEESSISSSISISIISSIISSIISSSSHHHSSSSIISS...</td>\n      <td>107</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>id_00181fd34</td>\n      <td>GGAAAGGAUCUCUAUCGAAGGAUAGAGAUCGCUCGCGACGGCACGA...</td>\n      <td>......((((((((((....))))))))))((((((..((.(((.....</td>\n      <td>EEEEEESSSSSSSSSSHHHHSSSSSSSSSSSSSSSSIISSISSSHH...</td>\n      <td>107</td>\n      <td>68</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>id_0020473f7</td>\n      <td>GGAAACCCGCCCGCGCCCGCCCGCGCUGCUGCCGUGCCUCCUCUCC...</td>\n      <td>.....(((((((((((((((((((((((((((((((((((((((((...</td>\n      <td>EEEEESSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS...</td>\n      <td>130</td>\n      <td>91</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(sub.shape)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-20T05:39:12.071572Z","iopub.execute_input":"2022-10-20T05:39:12.072363Z","iopub.status.idle":"2022-10-20T05:39:12.089291Z","shell.execute_reply.started":"2022-10-20T05:39:12.072327Z","shell.execute_reply":"2022-10-20T05:39:12.08821Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(457953, 6)\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"        id_seqpos  reactivity  deg_Mg_pH10  deg_pH10  deg_Mg_50C  deg_50C\n0  id_00073f8be_0         0.0          0.0       0.0         0.0      0.0\n1  id_00073f8be_1         0.0          0.0       0.0         0.0      0.0\n2  id_00073f8be_2         0.0          0.0       0.0         0.0      0.0\n3  id_00073f8be_3         0.0          0.0       0.0         0.0      0.0\n4  id_00073f8be_4         0.0          0.0       0.0         0.0      0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id_seqpos</th>\n      <th>reactivity</th>\n      <th>deg_Mg_pH10</th>\n      <th>deg_pH10</th>\n      <th>deg_Mg_50C</th>\n      <th>deg_50C</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>id_00073f8be_0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>id_00073f8be_1</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>id_00073f8be_2</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>id_00073f8be_3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>id_00073f8be_4</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## target","metadata":{}},{"cell_type":"code","source":"targets = list(sub.columns[1:])\nprint(targets)\n\ny_train = []\nseq_len = train[\"seq_length\"].iloc[0]\nseq_len_target = train[\"seq_scored\"].iloc[0]\nignore = -10000\nignore_length = seq_len - seq_len_target\nfor target in targets:\n    y = np.vstack(train[target])\n    dummy = np.zeros([y.shape[0], ignore_length]) + ignore\n    y = np.hstack([y, dummy])\n    y_train.append(y)\ny = np.stack(y_train, axis = 2)\ny.shape","metadata":{"execution":{"iopub.status.busy":"2022-10-20T05:39:12.093722Z","iopub.execute_input":"2022-10-20T05:39:12.094188Z","iopub.status.idle":"2022-10-20T05:39:12.181169Z","shell.execute_reply.started":"2022-10-20T05:39:12.094151Z","shell.execute_reply":"2022-10-20T05:39:12.180156Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(2096, 107, 5)"},"metadata":{}}]},{"cell_type":"markdown","source":"## structure adj","metadata":{}},{"cell_type":"code","source":"def get_structure_adj(train):\n    ## get adjacent matrix from structure sequence\n    \n    ## here I calculate adjacent matrix of each base pair, \n    ## but eventually ignore difference of base pair and integrate into one matrix\n    Ss = []\n    for i in tqdm(range(len(train))):\n        seq_length = train[\"seq_length\"].iloc[i]\n        structure = train[\"structure\"].iloc[i]\n        sequence = train[\"sequence\"].iloc[i]\n\n        cue = []\n        a_structures = {\n            (\"A\", \"U\") : np.zeros([seq_length, seq_length]),\n            (\"C\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"G\") : np.zeros([seq_length, seq_length]),\n            (\"U\", \"A\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"C\") : np.zeros([seq_length, seq_length]),\n            (\"G\", \"U\") : np.zeros([seq_length, seq_length]),\n        }\n        a_structure = np.zeros([seq_length, seq_length])\n        for i in range(seq_length):\n            if structure[i] == \"(\":\n                cue.append(i)\n            elif structure[i] == \")\":\n                start = cue.pop()\n#                 a_structure[start, i] = 1\n#                 a_structure[i, start] = 1\n                a_structures[(sequence[start], sequence[i])][start, i] = 1\n                a_structures[(sequence[i], sequence[start])][i, start] = 1\n        \n        a_strc = np.stack([a for a in a_structures.values()], axis = 2)\n        a_strc = np.sum(a_strc, axis = 2, keepdims = True)\n        Ss.append(a_strc)\n    \n    Ss = np.array(Ss)\n    print(Ss.shape)\n    return Ss\nSs = get_structure_adj(train)\nSs_pub = get_structure_adj(test_pub)\nSs_pri = get_structure_adj(test_pri)","metadata":{"execution":{"iopub.status.busy":"2022-10-20T05:39:12.182857Z","iopub.execute_input":"2022-10-20T05:39:12.183446Z","iopub.status.idle":"2022-10-20T05:39:16.441001Z","shell.execute_reply.started":"2022-10-20T05:39:12.183408Z","shell.execute_reply":"2022-10-20T05:39:16.440081Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=2096.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b22dd344d247444580bd6c094d4dbf93"}},"metadata":{}},{"name":"stdout","text":"\n(2096, 107, 107, 1)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=629.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2cb94eaf24043e7a0e580785ff3caa7"}},"metadata":{}},{"name":"stdout","text":"\n(629, 107, 107, 1)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=3005.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58bd07899e7d4a29a55364d8d55ff635"}},"metadata":{}},{"name":"stdout","text":"\n(3005, 130, 130, 1)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## distance adj","metadata":{}},{"cell_type":"code","source":"def get_distance_matrix(As):\n    ## adjacent matrix based on distance on the sequence\n    ## D[i, j] = 1 / (abs(i - j) + 1) ** pow, pow = 1, 2, 4\n    \n    idx = np.arange(As.shape[1])\n    Ds = []\n    for i in range(len(idx)):\n        d = np.abs(idx[i] - idx)\n        Ds.append(d)\n\n    Ds = np.array(Ds) + 1\n    Ds = 1/Ds\n    Ds = Ds[None, :,:]\n    Ds = np.repeat(Ds, len(As), axis = 0)\n    \n    Dss = []\n    for i in [1, 2, 4]: \n        Dss.append(Ds ** i)\n    Ds = np.stack(Dss, axis = 3)\n    print(Ds.shape)\n    return Ds\n\nDs = get_distance_matrix(As)\nDs_pub = get_distance_matrix(As_pub)\nDs_pri = get_distance_matrix(As_pri)","metadata":{"execution":{"iopub.status.busy":"2022-10-20T05:39:16.442398Z","iopub.execute_input":"2022-10-20T05:39:16.442747Z","iopub.status.idle":"2022-10-20T05:39:23.229857Z","shell.execute_reply.started":"2022-10-20T05:39:16.442715Z","shell.execute_reply":"2022-10-20T05:39:23.228982Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"(2096, 107, 107, 3)\n(629, 107, 107, 3)\n(3005, 130, 130, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"## concat adjecent\nAs = np.concatenate([As[:,:,:,None], Ss, Ds], axis = 3).astype(np.float32)\nAs_pub = np.concatenate([As_pub[:,:,:,None], Ss_pub, Ds_pub], axis = 3).astype(np.float32)\nAs_pri = np.concatenate([As_pri[:,:,:,None], Ss_pri, Ds_pri], axis = 3).astype(np.float32)\ndel Ss, Ds, Ss_pub, Ds_pub, Ss_pri, Ds_pri\nAs.shape, As_pub.shape, As_pri.shape","metadata":{"execution":{"iopub.status.busy":"2022-10-20T05:39:23.231268Z","iopub.execute_input":"2022-10-20T05:39:23.231847Z","iopub.status.idle":"2022-10-20T05:39:25.645028Z","shell.execute_reply.started":"2022-10-20T05:39:23.231808Z","shell.execute_reply":"2022-10-20T05:39:25.644056Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"((2096, 107, 107, 5), (629, 107, 107, 5), (3005, 130, 130, 5))"},"metadata":{}}]},{"cell_type":"markdown","source":"## node","metadata":{}},{"cell_type":"code","source":"## sequence\ndef return_ohe(n, i):\n    tmp = [0] * n\n    tmp[i] = 1\n    return tmp\n\ndef get_input(train):\n    ## get node features, which is one hot encoded\n    mapping = {}\n    vocab = [\"A\", \"G\", \"C\", \"U\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_node = np.stack(train[\"sequence\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n\n    mapping = {}\n    vocab = [\"S\", \"M\", \"I\", \"B\", \"H\", \"E\", \"X\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_loop = np.stack(train[\"predicted_loop_type\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    \n    mapping = {}\n    vocab = [\".\", \"(\", \")\"]\n    for i, s in enumerate(vocab):\n        mapping[s] = return_ohe(len(vocab), i)\n    X_structure = np.stack(train[\"structure\"].apply(lambda x : list(map(lambda y : mapping[y], list(x)))))\n    \n    \n    X_node = np.concatenate([X_node, X_loop], axis = 2)\n    \n    ## interaction\n    a = np.sum(X_node * (2 ** np.arange(X_node.shape[2])[None, None, :]), axis = 2)\n    vocab = sorted(set(a.flatten()))\n    print(vocab)\n    ohes = []\n    for v in vocab:\n        ohes.append(a == v)\n    ohes = np.stack(ohes, axis = 2)\n    X_node = np.concatenate([X_node, ohes], axis = 2).astype(np.float32)\n    \n    \n    print(X_node.shape)\n    return X_node\n\nX_node = get_input(train)\nX_node_pub = get_input(test_pub)\nX_node_pri = get_input(test_pri)","metadata":{"execution":{"iopub.status.busy":"2022-10-20T05:39:25.646554Z","iopub.execute_input":"2022-10-20T05:39:25.646902Z","iopub.status.idle":"2022-10-20T05:39:27.538883Z","shell.execute_reply.started":"2022-10-20T05:39:25.646864Z","shell.execute_reply":"2022-10-20T05:39:27.537712Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"[17, 18, 20, 24, 33, 34, 36, 40, 65, 66, 68, 72, 129, 130, 132, 136, 257, 258, 260, 264, 513, 514, 516, 520, 1025, 1026, 1028, 1032]\n(2096, 107, 39)\n[17, 18, 20, 24, 33, 34, 36, 40, 65, 66, 68, 72, 129, 130, 132, 136, 257, 258, 260, 264, 513, 514, 516, 520, 1025, 1026, 1028, 1032]\n(629, 107, 39)\n[17, 18, 20, 24, 33, 34, 36, 40, 65, 66, 68, 72, 129, 130, 132, 136, 257, 258, 260, 264, 513, 514, 516, 520, 1025, 1026, 1028, 1032]\n(3005, 130, 39)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## model","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers as L\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import backend as K\n\ndef mcrmse(t, p, seq_len_target = seq_len_target):\n    ## calculate mcrmse score by using numpy\n    t = t[:, :seq_len_target]\n    p = p[:, :seq_len_target]\n    \n    score = np.mean(np.sqrt(np.mean(np.mean((p - t) ** 2, axis = 1), axis = 0)))\n    return score\n\ndef mcrmse_loss(t, y, seq_len_target = seq_len_target):\n    ## calculate mcrmse score by using tf\n    t = t[:, :seq_len_target]\n    y = y[:, :seq_len_target]\n    \n    loss = tf.reduce_mean(tf.sqrt(tf.reduce_mean(tf.reduce_mean((t - y) ** 2, axis = 1), axis = 0)))\n    return loss\n\ndef attention(x_inner, x_outer, n_factor, dropout):\n    x_Q =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_inner)\n    x_K =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_outer)\n    x_V =  L.Conv1D(n_factor, 1, activation='linear', \n                  kernel_initializer='glorot_uniform',\n                  bias_initializer='glorot_uniform',\n                 )(x_outer)\n    x_KT = L.Permute((2, 1))(x_K)\n    res = L.Lambda(lambda c: K.batch_dot(c[0], c[1]) / np.sqrt(n_factor))([x_Q, x_KT])\n#     res = tf.expand_dims(res, axis = 3)\n#     res = L.Conv2D(16, 3, 1, padding = \"same\", activation = \"relu\")(res)\n#     res = L.Conv2D(1, 3, 1, padding = \"same\", activation = \"relu\")(res)\n#     res = tf.squeeze(res, axis = 3)\n    att = L.Lambda(lambda c: K.softmax(c, axis=-1))(res)\n    att = L.Lambda(lambda c: K.batch_dot(c[0], c[1]))([att, x_V])\n    return att\n\ndef multi_head_attention(x, y, n_factor, n_head, dropout):\n    if n_head == 1:\n        att = attention(x, y, n_factor, dropout)\n    else:\n        n_factor_head = n_factor // n_head\n        heads = [attention(x, y, n_factor_head, dropout) for i in range(n_head)]\n        att = L.Concatenate()(heads)\n        att = L.Dense(n_factor, \n                      kernel_initializer='glorot_uniform',\n                      bias_initializer='glorot_uniform',\n                     )(att)\n    x = L.Add()([x, att])\n    x = L.LayerNormalization()(x)\n    if dropout > 0:\n        x = L.Dropout(dropout)(x)\n    return x\n\ndef res(x, unit, kernel = 3, rate = 0.1):\n    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n    h = L.LayerNormalization()(h)\n    h = L.LeakyReLU()(h)\n    h = L.Dropout(rate)(h)\n    return L.Add()([x, h])\n\ndef forward(x, unit, kernel = 3, rate = 0.1):\n#     h = L.Dense(unit, None)(x)\n    h = L.Conv1D(unit, kernel, 1, padding = \"same\", activation = None)(x)\n    h = L.LayerNormalization()(h)\n    h = L.Dropout(rate)(h)\n#         h = tf.keras.activations.swish(h)\n    h = L.LeakyReLU()(h)\n    h = res(h, unit, kernel, rate)\n    return h\n\ndef adj_attn(x, adj, unit, n = 2, rate = 0.1):\n    x_a = x\n    x_as = []\n    for i in range(n):\n        x_a = forward(x_a, unit)\n        x_a = tf.matmul(adj, x_a) ## aggregate neighborhoods\n        x_as.append(x_a)\n    if n == 1:\n        x_a = x_as[0]\n    else:\n        x_a = L.Concatenate()(x_as)\n    x_a = forward(x_a, unit)\n    return x_a\n\n\ndef get_base(config):\n    ## base model architecture \n    ## node, adj -> middle feature\n    \n    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n    adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n    \n    adj_learned = L.Dense(1, \"relu\")(adj)\n    adj_all = L.Concatenate(axis = 3)([adj, adj_learned])\n        \n    xs = []\n    xs.append(node)\n    x1 = forward(node, 128, kernel = 3, rate = 0.0)\n    x2 = forward(x1, 64, kernel = 6, rate = 0.0)\n    x3 = forward(x2, 32, kernel = 15, rate = 0.0)\n    x4 = forward(x3, 16, kernel = 30, rate = 0.0)\n    x = L.Concatenate()([x1, x2, x3, x4])\n    \n    for unit in [64, 32]:\n        x_as = []\n        for i in range(adj_all.shape[3]):\n            x_a = adj_attn(x, adj_all[:, :, :, i], unit, rate = 0.0)\n            x_as.append(x_a)\n        x_c = forward(x, unit, kernel = 30)\n        \n        x = L.Concatenate()(x_as + [x_c])\n        x = forward(x, unit)\n        x = multi_head_attention(x, x, unit, 4, 0.0)\n        xs.append(x)\n        \n    x = L.Concatenate()(xs)\n\n    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n    return model\n\n\ndef get_ae_model(base, config):\n    ## denoising auto encoder part\n    ## node, adj -> middle feature -> node\n    \n    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n    adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n\n    x = base([L.SpatialDropout1D(0.3)(node), adj])\n    x = forward(x, 64, rate = 0.3)\n    p = L.Dense(X_node.shape[2], \"sigmoid\")(x)\n    \n    loss = - tf.reduce_mean(20 * node * tf.math.log(p + 1e-4) + (1 - node) * tf.math.log(1 - p + 1e-4))\n    model = tf.keras.Model(inputs = [node, adj], outputs = [loss])\n    \n    opt = get_optimizer()\n    model.compile(optimizer = opt, loss = lambda t, y : y)\n    return model\n\n\ndef get_model(base, config):\n    ## regression part\n    ## node, adj -> middle feature -> prediction of targets\n    \n    node = tf.keras.Input(shape = (None, X_node.shape[2]), name = \"node\")\n    adj = tf.keras.Input(shape = (None, None, As.shape[3]), name = \"adj\")\n    \n    x = base([node, adj])\n    x = forward(x, 128, rate = 0.4)\n    x = L.Dense(5, None)(x)\n\n    model = tf.keras.Model(inputs = [node, adj], outputs = [x])\n    \n    opt = get_optimizer()\n    model.compile(optimizer = opt, loss = mcrmse_loss, metrics=['accuracy'])\n    return model\n\ndef get_optimizer():\n#     sgd = tf.keras.optimizers.SGD(0.05, momentum = 0.9, nesterov=True)\n    adam = tf.optimizers.Adam()\n#     radam = tfa.optimizers.RectifiedAdam()\n#     lookahead = tfa.optimizers.Lookahead(adam, sync_period=6)\n#     swa = tfa.optimizers.SWA(adam)\n    return adam","metadata":{"execution":{"iopub.status.busy":"2022-10-20T05:39:27.540292Z","iopub.execute_input":"2022-10-20T05:39:27.540632Z","iopub.status.idle":"2022-10-20T05:39:32.172309Z","shell.execute_reply.started":"2022-10-20T05:39:27.540595Z","shell.execute_reply":"2022-10-20T05:39:32.170197Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). \n The versions of TensorFlow you are currently using is 2.3.0 and is not supported. \nSome things might work, some things might not.\nIf you were to encounter a bug, do not file an issue.\nIf you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \nYou can find the compatibility matrix in TensorFlow Addon's readme:\nhttps://github.com/tensorflow/addons\n  UserWarning,\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## pretrain","metadata":{}},{"cell_type":"code","source":"## here train denoising auto encoder model using all data\n\nconfig = {} ## not use now\nif ae_epochs > 0:\n    base = get_base(config)\n    ae_model = get_ae_model(base, config)\n    ## TODO : simultaneous train\n    for i in range(ae_epochs//ae_epochs_each):\n        print(f\"------ {i} ------\")\n        print(\"--- train ---\")\n        ae_model.fit([X_node, As], [X_node[:,0]],\n                  epochs = ae_epochs_each,\n                  batch_size = ae_batch_size)\n        print(\"--- public ---\")\n        ae_model.fit([X_node_pub, As_pub], [X_node_pub[:,0]],\n                  epochs = ae_epochs_each,\n                  batch_size = ae_batch_size)\n        print(\"--- private ---\")\n        ae_model.fit([X_node_pri, As_pri], [X_node_pri[:,0]],\n                  epochs = ae_epochs_each,\n                  batch_size = ae_batch_size)\n        gc.collect()\n    print(\"****** save ae model ******\")\n    base.save_weights(\"./base_ae\")","metadata":{"execution":{"iopub.status.busy":"2022-10-20T05:39:32.173782Z","iopub.execute_input":"2022-10-20T05:39:32.174151Z","iopub.status.idle":"2022-10-20T05:45:52.222399Z","shell.execute_reply.started":"2022-10-20T05:39:32.174113Z","shell.execute_reply":"2022-10-20T05:45:52.221322Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"------ 0 ------\n--- train ---\nEpoch 1/5\n66/66 [==============================] - 6s 89ms/step - loss: 0.9170\nEpoch 2/5\n66/66 [==============================] - 5s 83ms/step - loss: 0.3265\nEpoch 3/5\n66/66 [==============================] - 5s 79ms/step - loss: 0.1673\nEpoch 4/5\n66/66 [==============================] - 5s 81ms/step - loss: 0.1134\nEpoch 5/5\n66/66 [==============================] - 5s 80ms/step - loss: 0.0836\n--- public ---\nEpoch 1/5\n20/20 [==============================] - 2s 116ms/step - loss: 0.0678\nEpoch 2/5\n20/20 [==============================] - 2s 77ms/step - loss: 0.0688\nEpoch 3/5\n20/20 [==============================] - 2s 81ms/step - loss: 0.0630\nEpoch 4/5\n20/20 [==============================] - 2s 77ms/step - loss: 0.0633\nEpoch 5/5\n20/20 [==============================] - 2s 77ms/step - loss: 0.0567\n--- private ---\nEpoch 1/5\n94/94 [==============================] - 10s 103ms/step - loss: 0.0563\nEpoch 2/5\n94/94 [==============================] - 9s 94ms/step - loss: 0.0424\nEpoch 3/5\n94/94 [==============================] - 9s 91ms/step - loss: 0.0395\nEpoch 4/5\n94/94 [==============================] - 9s 92ms/step - loss: 0.0321\nEpoch 5/5\n94/94 [==============================] - 9s 90ms/step - loss: 0.0288\n------ 1 ------\n--- train ---\nEpoch 1/5\n66/66 [==============================] - 5s 80ms/step - loss: 0.0250\nEpoch 2/5\n66/66 [==============================] - 5s 81ms/step - loss: 0.0260\nEpoch 3/5\n66/66 [==============================] - 5s 80ms/step - loss: 0.0308\nEpoch 4/5\n66/66 [==============================] - 5s 81ms/step - loss: 0.0233\nEpoch 5/5\n66/66 [==============================] - 5s 79ms/step - loss: 0.0194\n--- public ---\nEpoch 1/5\n20/20 [==============================] - 2s 78ms/step - loss: 0.0188\nEpoch 2/5\n20/20 [==============================] - 2s 77ms/step - loss: 0.0164\nEpoch 3/5\n20/20 [==============================] - 2s 79ms/step - loss: 0.0234\nEpoch 4/5\n20/20 [==============================] - 2s 76ms/step - loss: 0.0179\nEpoch 5/5\n20/20 [==============================] - 2s 78ms/step - loss: 0.0185\n--- private ---\nEpoch 1/5\n94/94 [==============================] - 9s 92ms/step - loss: 0.0193\nEpoch 2/5\n94/94 [==============================] - 9s 93ms/step - loss: 0.0190\nEpoch 3/5\n94/94 [==============================] - 9s 93ms/step - loss: 0.0167\nEpoch 4/5\n94/94 [==============================] - 9s 92ms/step - loss: 0.0173\nEpoch 5/5\n94/94 [==============================] - 9s 92ms/step - loss: 0.0150\n------ 2 ------\n--- train ---\nEpoch 1/5\n66/66 [==============================] - 5s 80ms/step - loss: 0.0150\nEpoch 2/5\n66/66 [==============================] - 5s 79ms/step - loss: 0.0126\nEpoch 3/5\n66/66 [==============================] - 5s 78ms/step - loss: 0.0123\nEpoch 4/5\n66/66 [==============================] - 5s 81ms/step - loss: 0.0131\nEpoch 5/5\n66/66 [==============================] - 5s 77ms/step - loss: 0.0118\n--- public ---\nEpoch 1/5\n20/20 [==============================] - 2s 79ms/step - loss: 0.0122\nEpoch 2/5\n20/20 [==============================] - 2s 76ms/step - loss: 0.0093\nEpoch 3/5\n20/20 [==============================] - 2s 75ms/step - loss: 0.0112\nEpoch 4/5\n20/20 [==============================] - 2s 76ms/step - loss: 0.0097\nEpoch 5/5\n20/20 [==============================] - 1s 75ms/step - loss: 0.0097\n--- private ---\nEpoch 1/5\n94/94 [==============================] - 9s 91ms/step - loss: 0.0129\nEpoch 2/5\n94/94 [==============================] - 9s 93ms/step - loss: 0.0136\nEpoch 3/5\n94/94 [==============================] - 9s 91ms/step - loss: 0.0119\nEpoch 4/5\n94/94 [==============================] - 9s 92ms/step - loss: 0.0107\nEpoch 5/5\n94/94 [==============================] - 9s 92ms/step - loss: 0.0120\n------ 3 ------\n--- train ---\nEpoch 1/5\n66/66 [==============================] - 5s 81ms/step - loss: 0.0096\nEpoch 2/5\n66/66 [==============================] - 5s 79ms/step - loss: 0.0095\nEpoch 3/5\n66/66 [==============================] - 5s 80ms/step - loss: 0.0094\nEpoch 4/5\n66/66 [==============================] - 5s 80ms/step - loss: 0.0093\nEpoch 5/5\n66/66 [==============================] - 5s 81ms/step - loss: 0.0082\n--- public ---\nEpoch 1/5\n20/20 [==============================] - 2s 77ms/step - loss: 0.0087\nEpoch 2/5\n20/20 [==============================] - 2s 77ms/step - loss: 0.0093\nEpoch 3/5\n20/20 [==============================] - 2s 80ms/step - loss: 0.0075\nEpoch 4/5\n20/20 [==============================] - 2s 80ms/step - loss: 0.0086\nEpoch 5/5\n20/20 [==============================] - 2s 76ms/step - loss: 0.0091\n--- private ---\nEpoch 1/5\n94/94 [==============================] - 9s 90ms/step - loss: 0.0089\nEpoch 2/5\n94/94 [==============================] - 9s 91ms/step - loss: 0.0097\nEpoch 3/5\n94/94 [==============================] - 9s 91ms/step - loss: 0.0088\nEpoch 4/5\n94/94 [==============================] - 9s 92ms/step - loss: 0.0091\nEpoch 5/5\n94/94 [==============================] - 8s 90ms/step - loss: 0.0073\n****** save ae model ******\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## train","metadata":{}},{"cell_type":"code","source":"## here train regression model from pretrain auto encoder model\n\nfrom sklearn.model_selection import KFold\nkfold = KFold(5, shuffle = True, random_state = 42)\n\nscores = []\ntrain_loss = []\nval_loss = []\ntrain_acc = []\nval_acc = []\n\npreds = np.zeros([len(X_node), X_node.shape[1], 5])\nfor i, (tr_idx, va_idx) in enumerate(kfold.split(X_node, As)):\n    print(f\"------ fold {i} start -----\")\n    print(f\"------ fold {i} start -----\")\n    print(f\"------ fold {i} start -----\")\n    X_node_tr = X_node[tr_idx]\n    X_node_va = X_node[va_idx]\n    As_tr = As[tr_idx]\n    As_va = As[va_idx]\n    y_tr = y[tr_idx]\n    y_va = y[va_idx]\n    \n    base = get_base(config)\n    if ae_epochs > 0:\n        print(\"****** load ae model ******\")\n        base.load_weights(\"./base_ae\")\n    model = get_model(base, config)\n    if pretrain_dir is not None:\n        d = f\"./model{i}\"\n        print(f\"--- load from {d} ---\")\n        model.load_weights(d)\n    for epochs, batch_size in zip(epochs_list, batch_size_list):\n        print(f\"epochs : {epochs}, batch_size : {batch_size}\")\n        history = model.fit([X_node_tr, As_tr], [y_tr],\n                  validation_data=([X_node_va, As_va], [y_va]),\n                  epochs = epochs,\n                  batch_size = batch_size, validation_freq = 1)\n        \n        train_loss += [round(l, 4) for l in history.history['loss']]\n        val_loss += [round(l, 4) for l in history.history['val_loss']]\n        train_acc += [round(l, 4) for l in history.history['accuracy']]\n        val_acc += [round(l, 4) for l in history.history['val_accuracy']]\n\n        \n    model.save_weights(f\"./model{i}\")\n    p = model.predict([X_node_va, As_va])\n    scores.append(mcrmse(y_va, p))\n    print(f\"fold {i}: mcrmse {scores[-1]}\")\n    preds[va_idx] = p\n    if one_fold:\n        break\n        \npd.to_pickle(preds, \"oof.pkl\")","metadata":{"execution":{"iopub.status.busy":"2022-10-20T05:45:52.22592Z","iopub.execute_input":"2022-10-20T05:45:52.226225Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"------ fold 0 start -----\n------ fold 0 start -----\n------ fold 0 start -----\n****** load ae model ******\nepochs : 30, batch_size : 8\nEpoch 1/30\n210/210 [==============================] - 17s 81ms/step - loss: 0.5438 - val_loss: 0.3228\nEpoch 2/30\n210/210 [==============================] - 11s 51ms/step - loss: 0.3351 - val_loss: 0.3005\nEpoch 3/30\n210/210 [==============================] - 10s 50ms/step - loss: 0.3057 - val_loss: 0.2860\nEpoch 4/30\n210/210 [==============================] - 11s 51ms/step - loss: 0.2868 - val_loss: 0.2675\nEpoch 5/30\n210/210 [==============================] - 11s 50ms/step - loss: 0.2787 - val_loss: 0.2679\nEpoch 6/30\n210/210 [==============================] - 11s 51ms/step - loss: 0.2697 - val_loss: 0.2590\nEpoch 7/30\n210/210 [==============================] - 11s 53ms/step - loss: 0.2642 - val_loss: 0.2593\nEpoch 8/30\n210/210 [==============================] - 11s 51ms/step - loss: 0.2609 - val_loss: 0.2544\nEpoch 9/30\n210/210 [==============================] - 11s 51ms/step - loss: 0.2559 - val_loss: 0.2569\nEpoch 10/30\n210/210 [==============================] - 11s 51ms/step - loss: 0.2517 - val_loss: 0.2491\nEpoch 11/30\n210/210 [==============================] - 11s 51ms/step - loss: 0.2469 - val_loss: 0.2488\nEpoch 12/30\n210/210 [==============================] - 11s 52ms/step - loss: 0.2436 - val_loss: 0.2446\nEpoch 13/30\n210/210 [==============================] - 11s 52ms/step - loss: 0.2421 - val_loss: 0.2449\nEpoch 14/30\n210/210 [==============================] - 11s 51ms/step - loss: 0.2367 - val_loss: 0.2425\nEpoch 15/30\n210/210 [==============================] - 11s 54ms/step - loss: 0.2368 - val_loss: 0.2421\nEpoch 16/30\n210/210 [==============================] - 11s 53ms/step - loss: 0.2336 - val_loss: 0.2415\nEpoch 17/30\n210/210 [==============================] - 11s 51ms/step - loss: 0.2302 - val_loss: 0.2414\nEpoch 18/30\n210/210 [==============================] - 11s 55ms/step - loss: 0.2292 - val_loss: 0.2379\nEpoch 19/30\n210/210 [==============================] - 11s 52ms/step - loss: 0.2271 - val_loss: 0.2382\nEpoch 20/30\n210/210 [==============================] - 11s 52ms/step - loss: 0.2229 - val_loss: 0.2403\nEpoch 21/30\n210/210 [==============================] - 11s 54ms/step - loss: 0.2221 - val_loss: 0.2376\nEpoch 22/30\n210/210 [==============================] - 11s 52ms/step - loss: 0.2201 - val_loss: 0.2380\nEpoch 23/30\n210/210 [==============================] - 11s 51ms/step - loss: 0.2184 - val_loss: 0.2357\nEpoch 24/30\n210/210 [==============================] - 11s 53ms/step - loss: 0.2180 - val_loss: 0.2361\nEpoch 25/30\n210/210 [==============================] - 11s 51ms/step - loss: 0.2159 - val_loss: 0.2343\nEpoch 26/30\n210/210 [==============================] - 11s 51ms/step - loss: 0.2130 - val_loss: 0.2364\nEpoch 27/30\n210/210 [==============================] - 11s 53ms/step - loss: 0.2128 - val_loss: 0.2347\nEpoch 28/30\n210/210 [==============================] - 10s 50ms/step - loss: 0.2104 - val_loss: 0.2319\nEpoch 29/30\n210/210 [==============================] - 11s 51ms/step - loss: 0.2098 - val_loss: 0.2331\nEpoch 30/30\n210/210 [==============================] - 11s 53ms/step - loss: 0.2087 - val_loss: 0.2355\n<tensorflow.python.keras.callbacks.History object at 0x7f345d5b2650>\n{'loss': [0.5437965393066406, 0.33510279655456543, 0.3056650459766388, 0.28683483600616455, 0.2787252962589264, 0.26974257826805115, 0.26420485973358154, 0.2609063982963562, 0.25589603185653687, 0.2516905665397644, 0.24692855775356293, 0.2435721606016159, 0.24211983382701874, 0.23674315214157104, 0.2368486225605011, 0.23360255360603333, 0.23019909858703613, 0.2291634976863861, 0.22714340686798096, 0.22291263937950134, 0.22209370136260986, 0.22012920677661896, 0.2183995246887207, 0.21802523732185364, 0.21592263877391815, 0.21304698288440704, 0.21280178427696228, 0.2104494720697403, 0.209807887673378, 0.2087155282497406], 'val_loss': [0.32278597354888916, 0.30052921175956726, 0.2860170006752014, 0.2675451636314392, 0.2678562104701996, 0.2589693069458008, 0.25933194160461426, 0.2544390559196472, 0.25693798065185547, 0.24911682307720184, 0.24878565967082977, 0.24458199739456177, 0.24488243460655212, 0.2425045371055603, 0.24206900596618652, 0.2414965182542801, 0.24136395752429962, 0.2379031926393509, 0.238229438662529, 0.24027077853679657, 0.23758240044116974, 0.238037109375, 0.23571036756038666, 0.23607246577739716, 0.23433348536491394, 0.2364427149295807, 0.2346753478050232, 0.23191416263580322, 0.2330508977174759, 0.23548249900341034]}\ndict_keys(['loss', 'val_loss'])\nepochs : 10, batch_size : 16\nEpoch 1/10\n105/105 [==============================] - 8s 73ms/step - loss: 0.2029 - val_loss: 0.2303\nEpoch 2/10\n105/105 [==============================] - 7s 65ms/step - loss: 0.1994 - val_loss: 0.2319\nEpoch 3/10\n105/105 [==============================] - 7s 64ms/step - loss: 0.1985 - val_loss: 0.2299\nEpoch 4/10\n105/105 [==============================] - 7s 68ms/step - loss: 0.1976 - val_loss: 0.2298\nEpoch 5/10\n105/105 [==============================] - 7s 65ms/step - loss: 0.1975 - val_loss: 0.2316\nEpoch 6/10\n105/105 [==============================] - 7s 64ms/step - loss: 0.1961 - val_loss: 0.2303\nEpoch 7/10\n105/105 [==============================] - 7s 66ms/step - loss: 0.1951 - val_loss: 0.2301\nEpoch 8/10\n105/105 [==============================] - 7s 68ms/step - loss: 0.1943 - val_loss: 0.2311\nEpoch 9/10\n105/105 [==============================] - 7s 66ms/step - loss: 0.1949 - val_loss: 0.2306\nEpoch 10/10\n105/105 [==============================] - 7s 66ms/step - loss: 0.1935 - val_loss: 0.2310\n<tensorflow.python.keras.callbacks.History object at 0x7f3440075450>\n{'loss': [0.2029193639755249, 0.19939780235290527, 0.19854913651943207, 0.19756275415420532, 0.1974770426750183, 0.1960732489824295, 0.19511985778808594, 0.19434794783592224, 0.1948975771665573, 0.19353073835372925], 'val_loss': [0.23034417629241943, 0.23190432786941528, 0.2299278974533081, 0.22978578507900238, 0.23156285285949707, 0.23029983043670654, 0.23007842898368835, 0.23107343912124634, 0.23056218028068542, 0.23101525008678436]}\ndict_keys(['loss', 'val_loss'])\nepochs : 3, batch_size : 32\nEpoch 1/3\n53/53 [==============================] - 5s 91ms/step - loss: 0.1910 - val_loss: 0.2311\nEpoch 2/3\n53/53 [==============================] - 5s 90ms/step - loss: 0.1883 - val_loss: 0.2291\nEpoch 3/3\n53/53 [==============================] - 5s 90ms/step - loss: 0.1871 - val_loss: 0.2287\n<tensorflow.python.keras.callbacks.History object at 0x7f344080c650>\n{'loss': [0.1910288780927658, 0.18830332159996033, 0.18708907067775726], 'val_loss': [0.2311081439256668, 0.22910244762897491, 0.22872160375118256]}\ndict_keys(['loss', 'val_loss'])\nepochs : 3, batch_size : 64\nEpoch 1/3\n27/27 [==============================] - 4s 149ms/step - loss: 0.1864 - val_loss: 0.2298\nEpoch 2/3\n27/27 [==============================] - 3s 125ms/step - loss: 0.1850 - val_loss: 0.2287\nEpoch 3/3\n27/27 [==============================] - 3s 125ms/step - loss: 0.1840 - val_loss: 0.2288\n<tensorflow.python.keras.callbacks.History object at 0x7f34407e16d0>\n{'loss': [0.18642891943454742, 0.1849677711725235, 0.18404319882392883], 'val_loss': [0.22979488968849182, 0.2287086844444275, 0.2287970334291458]}\ndict_keys(['loss', 'val_loss'])\nepochs : 5, batch_size : 128\nEpoch 1/5\n14/14 [==============================] - 3s 204ms/step - loss: 0.1838 - val_loss: 0.2286\nEpoch 2/5\n14/14 [==============================] - 3s 198ms/step - loss: 0.1834 - val_loss: 0.2289\nEpoch 3/5\n14/14 [==============================] - 3s 198ms/step - loss: 0.1831 - val_loss: 0.2282\nEpoch 4/5\n14/14 [==============================] - 3s 201ms/step - loss: 0.1827 - val_loss: 0.2289\nEpoch 5/5\n14/14 [==============================] - 3s 200ms/step - loss: 0.1822 - val_loss: 0.2288\n<tensorflow.python.keras.callbacks.History object at 0x7f3440809610>\n{'loss': [0.18377485871315002, 0.18342240154743195, 0.18309642374515533, 0.18273203074932098, 0.18221920728683472], 'val_loss': [0.2286147028207779, 0.2288733720779419, 0.22820478677749634, 0.22888725996017456, 0.2288396805524826]}\ndict_keys(['loss', 'val_loss'])\nepochs : 5, batch_size : 256\nEpoch 1/5\n7/7 [==============================] - 4s 637ms/step - loss: 0.1819 - val_loss: 0.2291\nEpoch 2/5\n7/7 [==============================] - 2s 344ms/step - loss: 0.1819 - val_loss: 0.2289\nEpoch 3/5\n7/7 [==============================] - 2s 347ms/step - loss: 0.1815 - val_loss: 0.2289\nEpoch 4/5\n7/7 [==============================] - 2s 344ms/step - loss: 0.1811 - val_loss: 0.2293\nEpoch 5/5\n7/7 [==============================] - 2s 338ms/step - loss: 0.1809 - val_loss: 0.2290\n<tensorflow.python.keras.callbacks.History object at 0x7f3440815190>\n{'loss': [0.18188446760177612, 0.18188442289829254, 0.18145792186260223, 0.1811162233352661, 0.18088793754577637], 'val_loss': [0.22906902432441711, 0.22892746329307556, 0.22894442081451416, 0.2292831391096115, 0.22898632287979126]}\ndict_keys(['loss', 'val_loss'])\nfold 0: mcrmse 0.22907994722539504\n------ fold 1 start -----\n------ fold 1 start -----\n------ fold 1 start -----\n****** load ae model ******\nepochs : 30, batch_size : 8\nEpoch 1/30\n210/210 [==============================] - 17s 80ms/step - loss: 0.5411 - val_loss: 0.3304\nEpoch 2/30\n210/210 [==============================] - 10s 48ms/step - loss: 0.3366 - val_loss: 0.3010\nEpoch 3/30\n210/210 [==============================] - 11s 51ms/step - loss: 0.3076 - val_loss: 0.2871\nEpoch 4/30\n210/210 [==============================] - 10s 48ms/step - loss: 0.2931 - val_loss: 0.2668\nEpoch 5/30\n210/210 [==============================] - 10s 48ms/step - loss: 0.2816 - val_loss: 0.2586\nEpoch 6/30\n210/210 [==============================] - 10s 50ms/step - loss: 0.2735 - val_loss: 0.2538\nEpoch 7/30\n210/210 [==============================] - 11s 51ms/step - loss: 0.2665 - val_loss: 0.2547\nEpoch 8/30\n210/210 [==============================] - 11s 51ms/step - loss: 0.2624 - val_loss: 0.2545\nEpoch 9/30\n210/210 [==============================] - 11s 52ms/step - loss: 0.2586 - val_loss: 0.2465\nEpoch 10/30\n210/210 [==============================] - 10s 49ms/step - loss: 0.2550 - val_loss: 0.2442\nEpoch 11/30\n210/210 [==============================] - 10s 50ms/step - loss: 0.2487 - val_loss: 0.2415\nEpoch 12/30\n164/210 [======================>.......] - ETA: 2s - loss: 0.2472","output_type":"stream"}]},{"cell_type":"code","source":"print(scores)\nprint('train_loss', train_loss)\nprint('val_loss', val_loss)\nprint('train_acc', train_acc)\nprint('val_acc', val_acc)\ntrain_loss = [0.5505, 0.3342, 0.306, 0.2898, 0.2789, 0.2712, 0.2647, 0.2607, 0.2559, 0.2511, 0.2506, 0.2437, 0.2407, 0.2381, 0.2355, 0.2363, 0.2308, 0.2271, 0.2258, 0.2236, 0.2219, 0.2204, 0.2188, 0.217, 0.215, 0.2135, 0.2128, 0.2112, 0.2098, 0.2084, 0.2023, 0.1993, 0.1983, 0.1981, 0.1957, 0.1962, 0.1951, 0.1946, 0.1941, 0.1934, 0.1903, 0.1885, 0.1874, 0.1864, 0.1852, 0.1839, 0.1838, 0.1829, 0.1823, 0.1823, 0.1821, 0.1818, 0.1815, 0.1811, 0.1808, 0.1806, 0.5515, 0.3396, 0.3069, 0.2893, 0.2796, 0.2727, 0.2664, 0.261, 0.2572, 0.2533, 0.2471, 0.245, 0.2431, 0.2388, 0.237, 0.2343, 0.231, 0.2309, 0.2294, 0.2253, 0.2237, 0.2233, 0.2201, 0.2188, 0.2172, 0.2166, 0.2142, 0.2126, 0.2109, 0.2114, 0.2046, 0.2014, 0.2011, 0.1999, 0.1987, 0.1987, 0.1978, 0.1971, 0.1962, 0.1962, 0.1935, 0.1905, 0.1896, 0.1886, 0.1874, 0.1861, 0.1854, 0.1853, 0.1848, 0.1847, 0.1844, 0.1844, 0.184, 0.1835, 0.1829, 0.1826, 0.5469, 0.3354, 0.3046, 0.2872, 0.2763, 0.2674, 0.2625, 0.2576, 0.2537, 0.2488, 0.2441, 0.2421, 0.2387, 0.2368, 0.2341, 0.2312, 0.2288, 0.2258, 0.2242, 0.224, 0.2217, 0.2197, 0.2167, 0.2158, 0.2147, 0.2135, 0.2118, 0.2107, 0.2086, 0.2064, 0.2016, 0.1976, 0.1975, 0.1968, 0.1963, 0.1951, 0.1948, 0.1937, 0.1934, 0.1935, 0.1903, 0.1883, 0.187, 0.1857, 0.1846, 0.1836, 0.1833, 0.1829, 0.1821, 0.1819, 0.1821, 0.1823, 0.1813, 0.181, 0.1806, 0.1804, 0.5532, 0.3303, 0.3001, 0.2863, 0.2769, 0.2701, 0.2638, 0.259, 0.254, 0.251, 0.2463, 0.2432, 0.2401, 0.2363, 0.2336, 0.2309, 0.2294, 0.2289, 0.2243, 0.2237, 0.2209, 0.2194, 0.2182, 0.2147, 0.2132, 0.2132, 0.2108, 0.209, 0.2082, 0.2081, 0.2026, 0.1987, 0.1977, 0.1972, 0.1958, 0.1955, 0.195, 0.1947, 0.1948, 0.193, 0.19, 0.1876, 0.1876, 0.1866, 0.1848, 0.1841, 0.1833, 0.1834, 0.1824, 0.183, 0.1823, 0.1819, 0.1817, 0.1813, 0.181, 0.181, 0.5815, 0.344, 0.3127, 0.2918, 0.2808, 0.2734, 0.2672, 0.2612, 0.2574, 0.2525, 0.2489, 0.2452, 0.2432, 0.2381, 0.2369, 0.2341, 0.2325, 0.2288, 0.2275, 0.2239, 0.2232, 0.2217, 0.2193, 0.2182, 0.2165, 0.2154, 0.2138, 0.2116, 0.2111, 0.2085, 0.204, 0.1997, 0.1985, 0.198, 0.1975, 0.1963, 0.1967, 0.1948, 0.1942, 0.1949, 0.192, 0.189, 0.1886, 0.1871, 0.1858, 0.185, 0.1843, 0.1836, 0.1833, 0.183, 0.1833, 0.1825, 0.1824, 0.1822, 0.1817, 0.1816]\nval_loss = [0.3299, 0.3068, 0.2893, 0.2676, 0.2731, 0.2575, 0.2604, 0.2585, 0.2507, 0.2533, 0.2453, 0.2456, 0.2448, 0.2442, 0.241, 0.2481, 0.2389, 0.2406, 0.2401, 0.2482, 0.2376, 0.2355, 0.2372, 0.241, 0.235, 0.2343, 0.2333, 0.2373, 0.2376, 0.2372, 0.2334, 0.2323, 0.2331, 0.2307, 0.233, 0.2319, 0.2323, 0.2328, 0.2312, 0.2318, 0.2299, 0.2303, 0.2321, 0.23, 0.23, 0.2303, 0.2306, 0.23, 0.23, 0.2302, 0.2301, 0.2303, 0.23, 0.2304, 0.23, 0.2298, 0.3237, 0.2968, 0.2742, 0.2653, 0.2633, 0.2663, 0.2546, 0.2468, 0.2444, 0.2414, 0.2428, 0.24, 0.2391, 0.2407, 0.2385, 0.2336, 0.2336, 0.2365, 0.2344, 0.2372, 0.2343, 0.2299, 0.231, 0.2365, 0.2296, 0.2309, 0.2282, 0.2279, 0.2263, 0.2258, 0.2239, 0.2254, 0.2286, 0.2239, 0.2225, 0.2253, 0.2247, 0.2239, 0.2238, 0.227, 0.223, 0.2218, 0.2228, 0.2217, 0.2215, 0.2206, 0.2209, 0.2212, 0.2214, 0.2217, 0.221, 0.2214, 0.2214, 0.2216, 0.2209, 0.2209, 0.3368, 0.3088, 0.288, 0.2768, 0.2706, 0.2646, 0.2631, 0.2638, 0.2634, 0.2562, 0.2523, 0.2487, 0.25, 0.2463, 0.2503, 0.2488, 0.2479, 0.2486, 0.2462, 0.2488, 0.2449, 0.2396, 0.2402, 0.2426, 0.2492, 0.239, 0.2405, 0.2393, 0.2392, 0.2411, 0.236, 0.237, 0.2367, 0.2373, 0.2389, 0.2356, 0.2363, 0.2352, 0.2391, 0.2378, 0.2348, 0.2353, 0.2346, 0.2347, 0.2356, 0.2348, 0.2356, 0.2357, 0.2352, 0.2356, 0.2356, 0.2358, 0.2355, 0.2352, 0.2351, 0.2353, 0.3342, 0.2976, 0.2829, 0.2766, 0.2708, 0.2703, 0.269, 0.2654, 0.258, 0.2613, 0.252, 0.2523, 0.2515, 0.2505, 0.2534, 0.2491, 0.258, 0.2482, 0.2483, 0.2495, 0.2489, 0.245, 0.2436, 0.246, 0.2463, 0.2428, 0.2438, 0.2442, 0.2405, 0.2423, 0.2403, 0.2386, 0.2384, 0.2395, 0.2382, 0.2383, 0.2385, 0.2405, 0.2389, 0.2377, 0.2385, 0.2372, 0.2378, 0.2381, 0.2375, 0.237, 0.238, 0.2394, 0.2374, 0.2386, 0.2378, 0.238, 0.2377, 0.2378, 0.2381, 0.2383, 0.3388, 0.3083, 0.2789, 0.2765, 0.2655, 0.2592, 0.2566, 0.252, 0.257, 0.2488, 0.2526, 0.2451, 0.2473, 0.245, 0.252, 0.241, 0.24, 0.2377, 0.2388, 0.2385, 0.2393, 0.2356, 0.237, 0.2383, 0.2347, 0.2347, 0.2346, 0.2358, 0.234, 0.2319, 0.2311, 0.2305, 0.2307, 0.2316, 0.2317, 0.2304, 0.2307, 0.2301, 0.2325, 0.2312, 0.2299, 0.2337, 0.2309, 0.2303, 0.2296, 0.2294, 0.2299, 0.2302, 0.2299, 0.23, 0.2304, 0.2308, 0.2307, 0.2305, 0.2309, 0.231]\n# fig = px.line(\n#     history.history, y=['loss', 'val_loss'],\n#     labels={'index': 'epoch', 'value': 'MCRMSE'}, \n#     title='Training History')\n# fig.show()\n\nplt.plot(range(sum(epochs_list)), train_loss, val_loss)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## predict","metadata":{}},{"cell_type":"code","source":"p_pub = 0\np_pri = 0\nfor i in range(5):\n    model.load_weights(f\"./model{i}\")\n    p_pub += model.predict([X_node_pub, As_pub]) / 5\n    p_pri += model.predict([X_node_pri, As_pri]) / 5\n    if one_fold:\n        p_pub *= 5\n        p_pri *= 5\n        break\n\nfor i, target in enumerate(targets):\n    test_pub[target] = [list(p_pub[k, :, i]) for k in range(p_pub.shape[0])]\n    test_pri[target] = [list(p_pri[k, :, i]) for k in range(p_pri.shape[0])]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## sub","metadata":{}},{"cell_type":"code","source":"preds_ls = []\nfor df, preds in [(test_pub, p_pub), (test_pri, p_pri)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=targets)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_ls.append(single_df)\n\npreds_df = pd.concat(preds_ls)\npreds_df.to_csv(\"submission.csv\", index = False)\npreds_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(scores)\nprint(np.mean(scores))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}